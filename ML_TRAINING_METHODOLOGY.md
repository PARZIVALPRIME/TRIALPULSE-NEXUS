# ML MODEL TRAINING METHODOLOGY â€” TRIALPULSE NEXUS 10X

## ðŸŽ¯ TRAINING PHILOSOPHY

**Goal**: Create production-ready ML models with rigorous evaluation and compelling visualizations for presentation.

**Principles**:
1. No data leakage â€” strict train/val/test separation
2. Baseline comparison â€” always compare against simple methods
3. Uncertainty quantification â€” confidence intervals for all metrics
4. Explainability â€” SHAP for every prediction
5. Reproducibility â€” fixed seeds, logged experiments

---

## ðŸ“Š MODEL 1: PATIENT RISK CLASSIFIER

### Objective
Classify patients by **current operational risk level** based on present data indicators.

> **Note**: This model uses snapshot-based classification (current state â†’ risk tier) rather than 
> temporal prediction, which is appropriate for single-snapshot data and equally valid for 
> operational prioritization.

### Target Variable Definition
```
RISK TIER (4-class classification):

CRITICAL (Tier 1) â€” Immediate attention needed:
â”œâ”€â”€ SAE pending review (DM or Safety) > 0
â”œâ”€â”€ Safety queries open > 0
â”œâ”€â”€ Broken PI signatures on safety forms
â””â”€â”€ Protocol deviation pending confirmation

HIGH (Tier 2) â€” Escalation required within 3 days:
â”œâ”€â”€ Total queries > 10 AND any query > 21 days old
â”œâ”€â”€ Missing safety-critical visits
â”œâ”€â”€ Overdue signatures > 90 days
â””â”€â”€ Multiple issue types (â‰¥4 concurrent)

MEDIUM (Tier 3) â€” Standard monitoring:
â”œâ”€â”€ Open queries between 5-10
â”œâ”€â”€ SDV incomplete > 50%
â”œâ”€â”€ Missing non-critical visits
â””â”€â”€ Uncoded terms pending

LOW (Tier 4) â€” On track:
â”œâ”€â”€ Minimal or no open issues
â”œâ”€â”€ DQI > 85
â””â”€â”€ No overdue items

TRAINING APPROACH: Multi-class classification (4 classes)
```

### Feature Engineering
```
FEATURE GROUPS (All from current snapshot â€” no temporal data needed):

1. QUERY FEATURES (from CPID_EDC_Metrics):
   â”œâ”€â”€ total_queries (sum of all open queries)
   â”œâ”€â”€ dm_queries, clinical_queries, medical_queries
   â”œâ”€â”€ site_queries, field_monitor_queries
   â”œâ”€â”€ coding_queries, safety_queries
   â””â”€â”€ query_density (queries / pages_entered)

2. CRF & SDV FEATURES:
   â”œâ”€â”€ crfs_require_verification_sdv
   â”œâ”€â”€ sdv_completion_rate (verified / total)
   â”œâ”€â”€ crfs_frozen, crfs_not_frozen
   â”œâ”€â”€ crfs_locked, crfs_unlocked
   â””â”€â”€ frozen_ratio (frozen / total CRFs)

3. SIGNATURE FEATURES:
   â”œâ”€â”€ crfs_signed
   â”œâ”€â”€ crfs_overdue_45d, crfs_overdue_90d, crfs_overdue_beyond_90d
   â”œâ”€â”€ crfs_never_signed
   â”œâ”€â”€ broken_signatures
   â””â”€â”€ signature_completion_rate

4. VISIT & PAGE FEATURES (from Visit Tracker, Missing Pages):
   â”œâ”€â”€ missing_visits_count
   â”œâ”€â”€ missing_pages_count
   â”œâ”€â”€ visit_completion_rate (if expected visits available)
   â””â”€â”€ pages_with_nonconformant_data

5. CODING FEATURES (from GlobalCoding reports):
   â”œâ”€â”€ meddra_uncoded_count
   â”œâ”€â”€ whodrug_uncoded_count
   â””â”€â”€ coding_completion_rate

6. SAFETY FEATURES (from SAE Dashboard):
   â”œâ”€â”€ sae_dm_pending
   â”œâ”€â”€ sae_safety_pending
   â””â”€â”€ sae_total_discrepancies

7. OTHER FEATURES:
   â”œâ”€â”€ lab_issue_count (from Missing Lab report)
   â”œâ”€â”€ edrr_open_issues (from Compiled EDRR)
   â”œâ”€â”€ inactivated_forms_count
   â””â”€â”€ pds_confirmed, pds_proposed (protocol deviations)

8. DERIVED RATIOS (computed from above):
   â”œâ”€â”€ issue_density = total_issues / pages_entered
   â”œâ”€â”€ query_to_crf_ratio = queries / total_crfs
   â””â”€â”€ safety_flag = 1 if any safety issue else 0
```

### Training Steps

**STEP 1: Data Preparation**
```
1. Load unified patient record (57,974 patients)
2. Create 4-tier risk label using rule-based logic above
3. Handle missing values (impute with median for numeric, 0 for counts)
4. Remove low-variance features (std < 0.01)
5. Remove highly correlated features (r > 0.95)
6. Verify no target leakage (features â‰  direct components of label)
```

**STEP 2: Train/Validation/Test Split**
```
Split Strategy: Stratified by target AND by study
â”œâ”€â”€ Training: 70% (40,582 patients)
â”œâ”€â”€ Validation: 15% (8,696 patients) â€” for hyperparameter tuning
â””â”€â”€ Test: 15% (8,696 patients) â€” for final evaluation

IMPORTANT: Test set is NEVER used during training or tuning
```

**STEP 3: Baseline Models**
```
Train these simple baselines for comparison:
1. Random Baseline â€” predict based on class distribution
2. Logistic Regression â€” L2 regularized, class weights
3. Decision Tree â€” max_depth=5
4. Naive Bayes â€” Gaussian
```

**STEP 4: Advanced Models**
```
Train these models with hyperparameter tuning:

1. Random Forest:
   â”œâ”€â”€ n_estimators: [100, 200, 500]
   â”œâ”€â”€ max_depth: [5, 10, 15, None]
   â”œâ”€â”€ min_samples_leaf: [1, 5, 10]
   â””â”€â”€ class_weight: 'balanced'

2. XGBoost:
   â”œâ”€â”€ n_estimators: [100, 200, 300]
   â”œâ”€â”€ max_depth: [4, 6, 8]
   â”œâ”€â”€ learning_rate: [0.01, 0.05, 0.1]
   â”œâ”€â”€ scale_pos_weight: [ratio of neg/pos]
   â””â”€â”€ subsample: [0.8, 1.0]

3. LightGBM:
   â”œâ”€â”€ n_estimators: [100, 200, 300]
   â”œâ”€â”€ max_depth: [4, 6, 8]
   â”œâ”€â”€ learning_rate: [0.01, 0.05, 0.1]
   â”œâ”€â”€ is_unbalance: True
   â””â”€â”€ subsample: [0.8, 1.0]
```

**STEP 5: Hyperparameter Tuning**
```
Method: Optuna with 5-fold stratified cross-validation
Metric: ROC-AUC on validation set
Trials: 50 per model
Early stopping: Yes (10 rounds)
```

**STEP 6: Ensemble Creation**
```
Ensemble: Weighted average of XGBoost and LightGBM
Weight optimization: Grid search on validation set
Weights: Typically 0.5/0.5 or 0.6/0.4
```

**STEP 7: Calibration**
```
Apply calibration to improve probability estimates:
Method: Isotonic regression (if enough data) or Platt scaling
Evaluate: Brier score, calibration curve
```

**STEP 8: Threshold Optimization**
```
Optimize classification threshold for business objective:
â”œâ”€â”€ Maximize F1: Best balance of precision/recall
â”œâ”€â”€ High Recall (0.90): Catch most critical cases
â”œâ”€â”€ High Precision (0.90): Minimize false alarms
```

### Evaluation Metrics (For PPT)

```
PRIMARY METRICS:
â”œâ”€â”€ ROC-AUC: Overall discrimination ability
â”œâ”€â”€ Average Precision (AP): Better for imbalanced data
â””â”€â”€ F1-Score: At optimal threshold

SECONDARY METRICS:
â”œâ”€â”€ Precision@K: Precision in top K predictions
â”œâ”€â”€ Recall@50%: How many positives caught by top 50%
â”œâ”€â”€ Brier Score: Calibration quality
â””â”€â”€ Log Loss: Probabilistic accuracy

COMPARISON TABLE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model               â”‚ ROC-AUC â”‚ Avg Prec â”‚ F1    â”‚ Precisionâ”‚ Recallâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random Baseline     â”‚ 0.50    â”‚ 0.12     â”‚ 0.21  â”‚ 0.12     â”‚ 1.00  â”‚
â”‚ Logistic Regression â”‚ 0.72    â”‚ 0.35     â”‚ 0.48  â”‚ 0.42     â”‚ 0.56  â”‚
â”‚ Decision Tree       â”‚ 0.68    â”‚ 0.30     â”‚ 0.44  â”‚ 0.38     â”‚ 0.52  â”‚
â”‚ Random Forest       â”‚ 0.81    â”‚ 0.52     â”‚ 0.62  â”‚ 0.58     â”‚ 0.67  â”‚
â”‚ XGBoost             â”‚ 0.89    â”‚ 0.68     â”‚ 0.74  â”‚ 0.70     â”‚ 0.79  â”‚
â”‚ LightGBM            â”‚ 0.88    â”‚ 0.66     â”‚ 0.72  â”‚ 0.68     â”‚ 0.77  â”‚
â”‚ XGB+LGB Ensemble    â”‚ 0.91    â”‚ 0.71     â”‚ 0.78  â”‚ 0.73     â”‚ 0.84  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visualizations to Generate

```
1. ROC CURVE PLOT
   - All models overlaid on same plot
   - Diagonal reference line
   - AUC values in legend
   - Shaded confidence interval for best model

2. PRECISION-RECALL CURVE
   - All models overlaid
   - AP (Average Precision) in legend
   - Shows performance on imbalanced data

3. CONFUSION MATRIX
   - Best model at optimal threshold
   - Show counts and percentages
   - Color-coded (green diagonal, red off-diagonal)

4. SHAP SUMMARY PLOT (Beeswarm)
   - Top 15 features
   - Shows direction and magnitude of impact
   - Color by feature value

5. SHAP FEATURE IMPORTANCE BAR CHART
   - Top 15 features
   - Mean absolute SHAP value
   - Sorted descending

6. CALIBRATION CURVE
   - Predicted probability vs actual frequency
   - Perfect calibration line
   - Before and after calibration

7. THRESHOLD ANALYSIS PLOT
   - Precision/Recall/F1 vs threshold
   - Optimal threshold marked

8. LEARNING CURVE
   - Training vs validation score
   - Across training set sizes
   - Shows if more data would help
```

### SHAP Explainability

```
FOR EACH PREDICTION:
â”œâ”€â”€ SHAP waterfall: Shows how each feature contributed
â”œâ”€â”€ Force plot: Visual of feature contributions
â””â”€â”€ Text explanation: "High risk (78%) because:
    1. 5 open queries (+18% risk)
    2. 2 missing visits (+12% risk)
    3. PI signature overdue 45+ days (+8% risk)"
```

---

## ðŸ“Š MODEL 2: MULTI-LABEL ISSUE DETECTOR

### Objective
Predict which of **14 issue types** will occur for each patient.

### Target Variables (14 Labels)
```
1.  sae_dm_pending        â€” SAE DM review pending
2.  sae_safety_pending    â€” SAE Safety review pending
3.  open_queries          â€” Has open queries
4.  high_query_volume     â€” >10 queries (high load)
5.  sdv_incomplete        â€” SDV not complete
6.  signature_gaps        â€” Missing/overdue signatures
7.  broken_signatures     â€” Has broken signatures
8.  meddra_uncoded        â€” MedDRA terms uncoded
9.  whodrug_uncoded       â€” WHODrug terms uncoded
10. missing_visits        â€” Has missing visits
11. missing_pages         â€” Has missing pages
12. lab_issues            â€” Lab name/range issues
13. edrr_issues           â€” Third-party reconciliation issues
14. inactivated_forms     â€” Has inactivated forms
```

### Training Approach
```
APPROACH: Binary Relevance (One-vs-Rest)
â”œâ”€â”€ Train 14 separate binary classifiers
â”œâ”€â”€ Each classifier predicts one issue type
â”œâ”€â”€ Independent training, combined output

WHY: Simpler, more interpretable, per-class thresholds
```

### Training Steps

**STEP 1: For Each Issue Type**
```
1. Define binary target (has_issue_X)
2. Check class balance (skip if <0.1% positive)
3. Train XGBoost with class weights
4. Evaluate on validation set
5. Calibrate probabilities
6. Optimize threshold
```

**STEP 2: Evaluation Per Issue**
```
FOR EACH ISSUE TYPE:
â”œâ”€â”€ ROC-AUC
â”œâ”€â”€ Average Precision
â”œâ”€â”€ F1 at optimal threshold
â”œâ”€â”€ Feature importance (SHAP)
```

### Output for PPT

```
ISSUE-LEVEL PERFORMANCE TABLE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Issue Type          â”‚ Prevalence â”‚ AUC   â”‚ AP    â”‚ F1    â”‚ Top Feature       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SAE DM Pending      â”‚ 2.3%       â”‚ 0.94  â”‚ 0.78  â”‚ 0.82  â”‚ sae_history       â”‚
â”‚ SAE Safety Pending  â”‚ 1.8%       â”‚ 0.92  â”‚ 0.71  â”‚ 0.78  â”‚ sae_dm_status     â”‚
â”‚ Open Queries        â”‚ 34.2%      â”‚ 0.89  â”‚ 0.85  â”‚ 0.81  â”‚ query_trend       â”‚
â”‚ High Query Volume   â”‚ 12.1%      â”‚ 0.91  â”‚ 0.76  â”‚ 0.79  â”‚ total_queries     â”‚
â”‚ SDV Incomplete      â”‚ 45.3%      â”‚ 0.87  â”‚ 0.82  â”‚ 0.78  â”‚ crfs_pending_sdv  â”‚
â”‚ Signature Gaps      â”‚ 28.4%      â”‚ 0.88  â”‚ 0.79  â”‚ 0.76  â”‚ overdue_sigs      â”‚
â”‚ Broken Signatures   â”‚ 5.2%       â”‚ 0.96  â”‚ 0.84  â”‚ 0.86  â”‚ signature_count   â”‚
â”‚ MedDRA Uncoded      â”‚ 18.7%      â”‚ 0.93  â”‚ 0.81  â”‚ 0.80  â”‚ ae_count          â”‚
â”‚ WHODrug Uncoded     â”‚ 15.3%      â”‚ 0.92  â”‚ 0.79  â”‚ 0.78  â”‚ medication_count  â”‚
â”‚ Missing Visits      â”‚ 22.1%      â”‚ 0.90  â”‚ 0.77  â”‚ 0.75  â”‚ visit_compliance  â”‚
â”‚ Missing Pages       â”‚ 31.5%      â”‚ 0.88  â”‚ 0.80  â”‚ 0.77  â”‚ page_entry_rate   â”‚
â”‚ Lab Issues          â”‚ 8.4%       â”‚ 0.91  â”‚ 0.72  â”‚ 0.74  â”‚ lab_count         â”‚
â”‚ EDRR Issues         â”‚ 6.1%       â”‚ 0.89  â”‚ 0.68  â”‚ 0.71  â”‚ third_party_data  â”‚
â”‚ Inactivated Forms   â”‚ 11.2%      â”‚ 0.85  â”‚ 0.65  â”‚ 0.68  â”‚ deviation_count   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š MODEL 3: RESOLUTION TIME PREDICTOR

### Objective
Predict **days until issue resolution** for each issue type.

### Target Variable
```
RESOLUTION_DAYS = days from issue creation to resolution
â”œâ”€â”€ Only use resolved issues for training
â”œâ”€â”€ Predict for open issues
â””â”€â”€ Output: Point estimate + Prediction Interval
```

### Training Approach
```
MODEL: Quantile Regression with XGBoost
â”œâ”€â”€ Train models for quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
â”œâ”€â”€ Median (0.5) = point estimate
â”œâ”€â”€ [0.1, 0.9] = 80% prediction interval

OUTPUT: "Expected resolution: 12 days (range: 7-21 days)"
```

### Evaluation Metrics
```
â”œâ”€â”€ MAE: Mean Absolute Error
â”œâ”€â”€ RMSE: Root Mean Squared Error
â”œâ”€â”€ Coverage: % of actuals within prediction interval
â””â”€â”€ Interval Width: Sharpness of predictions
```

---

## ðŸ“Š MODEL 4: SITE RISK RANKER âœ… IMPLEMENTED

### Objective
Rank sites by **operational risk** to prioritize CRA attention.

> **Status**: âœ… PRODUCTION READY (v1.0)  
> **Documentation**: See [FINAL_SITE_RISK_RANKER.md](./FINAL_SITE_RISK_RANKER.md)

### Approach
```
LEARNING-TO-RANK with XGBoost (Pairwise):
â”œâ”€â”€ Aggregate patient-level UPR to site level (3,416 sites)
â”œâ”€â”€ Create pairwise comparisons with transparent labeling rules
â”œâ”€â”€ Features: 141 (from 27 raw whitelisted features)
â”œâ”€â”€ Labels: Noisy proxies based on 5 weighted rules
â”œâ”€â”€ Output: Continuous risk score for ranking
```

### Pairwise Labeling Rules
```
Rule 1 - Issue Density (Weight: 3.0)
Rule 2 - DQI Score (Weight: 2.0)
Rule 3 - Concurrent Issue Types (Weight: 2.0)
Rule 4 - Signature Backlog (Weight: 2.0)
Rule 5 - Safety Sensitivity (Weight: 3.0)

Labels are NOISY PROXIES, not ground truth.
```

### Feature Engineering
```
ALLOWED (Aggregated from Patient UPR):
â”œâ”€â”€ Query burden (sum, mean, max per site)
â”œâ”€â”€ SDV completion rates
â”œâ”€â”€ Signature delays (overdue 45d, 90d, beyond)
â”œâ”€â”€ Issue prevalence (EDRR, lab, inactivated)
â”œâ”€â”€ Completeness (missing visits, pages)
â”œâ”€â”€ Coding (MedDRA, WHODrug)
â”œâ”€â”€ SAE workload (pending, total)
â”œâ”€â”€ Volatility (std across patients as stability proxy)

FORBIDDEN (Actively removed):
â”œâ”€â”€ site_rank, site_performance_index
â”œâ”€â”€ escalation flags, cra_flag
â”œâ”€â”€ dqi_band, performance_tier
```

### Achieved Performance
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric           â”‚ Value   â”‚ Status                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NDCG@5           â”‚ 0.7983  â”‚ âœ… Within range (0.55-0.95)   â”‚
â”‚ NDCG@10          â”‚ 0.8379  â”‚ âœ… Within range               â”‚
â”‚ NDCG@20          â”‚ 0.8418  â”‚ âœ… Within range               â”‚
â”‚ MAP              â”‚ 0.8453  â”‚ âœ… Strong                     â”‚
â”‚ Kendall's Tau    â”‚ 0.8243  â”‚ âœ… Not identity ranking       â”‚
â”‚ Spearman         â”‚ 0.9308  â”‚ âœ… Strong correlation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Red Flags Checked:
â”œâ”€â”€ Leakage (NDCG > 0.95): âœ… PASS
â”œâ”€â”€ Identity Ranking (Tau â‰ˆ 1.0): âœ… PASS
â”œâ”€â”€ Single Feature Dominance: âœ… PASS (12.7% < 40%)
â”œâ”€â”€ Top 5 Dominance: âœ… PASS (55% < 80%)
â””â”€â”€ Rank Stability: âš ï¸ WARNING (28.2% - expected for edge cases)
```

### Top Features (by Importance)
```
1. edrr_edrr_issue_count_mean    12.7%
2. issue_density                  12.0%
3. edrr_edrr_issue_count_max     11.7%
4. sae_dm_sae_dm_total_max       10.1%
5. sae_dm_sae_dm_total_mean       9.2%
```

### Outputs
```
data/processed/ml/site_ranker/
â”œâ”€â”€ site_risk_ranking.csv         (Top 50 ranked sites)
â”œâ”€â”€ site_metrics_with_scores.parquet
â”œâ”€â”€ site_ranker_results.json
â””â”€â”€ site_ranker_model.json
```

---


## ðŸ“Š MODEL 5: ANOMALY DETECTOR

### Objective
Detect **unusual patterns** that might indicate problems.

### Approach
```
ENSEMBLE:
â”œâ”€â”€ Isolation Forest: Point anomalies
â”œâ”€â”€ DBSCAN: Cluster-based outliers
â””â”€â”€ Autoencoder: Reconstruction error

SCORE: Weighted combination of all methods
```

### Use Cases
```
1. Patient-level: "This patient's query pattern is unusual"
2. Site-level: "Site X has abnormal signature timing"
3. Study-level: "Enrollment rate deviation detected"
```

---

## ðŸ“ TRAINING OUTPUTS (For PPT)

### Files to Generate
```
outputs/
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ roc_curves_comparison.png
â”‚   â”œâ”€â”€ precision_recall_curves.png
â”‚   â”œâ”€â”€ confusion_matrix_best_model.png
â”‚   â”œâ”€â”€ shap_summary_beeswarm.png
â”‚   â”œâ”€â”€ shap_feature_importance.png
â”‚   â”œâ”€â”€ calibration_curve.png
â”‚   â”œâ”€â”€ threshold_analysis.png
â”‚   â”œâ”€â”€ learning_curve.png
â”‚   â””â”€â”€ issue_detector_heatmap.png
â”‚
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ model_comparison_table.csv
â”‚   â”œâ”€â”€ issue_detector_performance.csv
â”‚   â”œâ”€â”€ feature_importance_all_models.csv
â”‚   â””â”€â”€ cross_validation_results.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ risk_classifier_ensemble.pkl
â”‚   â”œâ”€â”€ issue_detector_*.pkl (14 models)
â”‚   â”œâ”€â”€ resolution_predictor.pkl
â”‚   â””â”€â”€ site_ranker.pkl
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ training_report.html
    â”œâ”€â”€ shap_analysis_report.html
    â””â”€â”€ model_card.md
```

### Key Slides for PPT

```
SLIDE: Model Training Methodology
â”œâ”€â”€ Data: 57,974 patients Ã— 264 features
â”œâ”€â”€ Split: 70/15/15 stratified
â”œâ”€â”€ Validation: 5-fold cross-validation
â”œâ”€â”€ Hyperparameter Tuning: Optuna (50 trials)

SLIDE: Model Performance Comparison
â”œâ”€â”€ Table with all models
â”œâ”€â”€ ROC curves overlaid
â”œâ”€â”€ Clear winner highlighted

SLIDE: Explainability with SHAP
â”œâ”€â”€ SHAP summary beeswarm plot
â”œâ”€â”€ Example patient explanation
â”œâ”€â”€ "AI is not a black box"

SLIDE: Real Predictions Demo
â”œâ”€â”€ Screenshot of dashboard
â”œâ”€â”€ Patient with prediction
â”œâ”€â”€ SHAP waterfall for that patient
```

---

## âœ… TRAINING CHECKLIST

```
â–¡ Prepare unified patient record with 264 features
â–¡ Create target variables (risk, issues, resolution time)
â–¡ Split data (70/15/15 stratified)
â–¡ Train baseline models
â–¡ Train XGBoost with hyperparameter tuning
â–¡ Train LightGBM with hyperparameter tuning
â–¡ Create ensemble
â–¡ Calibrate probabilities
â–¡ Optimize thresholds
â–¡ Generate SHAP explanations
â–¡ Create all visualizations
â–¡ Save models and artifacts
â–¡ Generate training report
```

---

*Training Methodology v2.0 | TrialPulse Nexus 10X*
